{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":51515,"databundleVersionId":5489915,"sourceType":"competition"},{"sourceId":5425887,"sourceType":"datasetVersion","datasetId":3140359},{"sourceId":8112798,"sourceType":"datasetVersion","datasetId":4792618}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import Dataset, load_metric\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\nlabeled_data_path = '/kaggle/input/datathon-2024/checkworthiness_labeled.csv'\nleaderboard_data_path = '/kaggle/input/datathon-2024/checkworthiness_leaderboard.csv'\nlabeled_data = pd.read_csv(labeled_data_path)\nleaderboard_data = pd.read_csv(leaderboard_data_path)\n\n# Preprocess and encode labels\ndef preprocess_text(text):\n    return text.lower()  # Convert to lowercase\n\nlabeled_data['Text'] = labeled_data['Text'].apply(preprocess_text)\nleaderboard_data['Text'] = leaderboard_data['Text'].apply(preprocess_text)\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\nlabeled_data['labels'] = label_encoder.fit_transform(labeled_data['Category'])\n\n# Load tokenizer and model\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label_encoder.classes_))\n\n# Tokenization function that includes truncation and padding\ndef tokenize_function(examples):\n    return tokenizer(examples['Text'], truncation=True, padding='max_length', max_length=128)\n\nfrom sklearn.model_selection import train_test_split\nfrom datasets import DatasetDict\nfrom datasets import Dataset\n\n# Convert to datasets and apply the tokenization\nfull_dataset = Dataset.from_pandas(labeled_data[['Text', 'labels']])\nfull_dataset = full_dataset.map(tokenize_function, batched=True)\n\n# Splitting the dataset into training and evaluation datasets\ntrain_test_split = full_dataset.train_test_split(test_size=0.1)\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n\n# Initialize the Trainer with eval_dataset\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer\n)\n\n# Train the model\ntrainer.train()\n\n# Tokenize the leaderboard dataset\nleaderboard_dataset = Dataset.from_pandas(leaderboard_data)\nleaderboard_dataset = leaderboard_dataset.map(tokenize_function, batched=True)\n\n# Make predictions\npredictions = trainer.predict(leaderboard_dataset)\npreds = predictions.predictions.argmax(-1)\n\n# Decode predictions\ndecoded_preds = label_encoder.inverse_transform(preds)\n\n# Prepare the submission file\nleaderboard_data['Category'] = decoded_preds\nleaderboard_data.to_csv('submission.csv', index=False)\nprint(\"The model has been trained and the predictions are saved in 'submission.csv'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T04:10:49.542006Z","iopub.execute_input":"2024-04-14T04:10:49.543127Z","iopub.status.idle":"2024-04-14T04:17:20.363250Z","shell.execute_reply.started":"2024-04-14T04:10:49.543078Z","shell.execute_reply":"2024-04-14T04:17:20.361477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install required libraries\n!pip install transformers datasets\n\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset (Assume it's already uploaded to Kaggle input directory)\nlabeled_data = pd.read_csv('/kaggle/input/datathon-2024/checkworthiness_labeled.csv')\nleaderboard_data = pd.read_csv('/kaggle/input/datathon-2024/checkworthiness_leaderboard.csv')\n\n# Preprocessing and Tokenization setup\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples['Text'], truncation=True, padding='max_length', max_length=128)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nlabeled_data['labels'] = label_encoder.fit_transform(labeled_data['Category'])\n\n# Dataset creation and split\nfull_dataset = Dataset.from_pandas(labeled_data[['Text', 'labels']])\nfull_dataset = full_dataset.map(tokenize_function, batched=True)\ntrain_test_split = full_dataset.train_test_split(test_size=0.1)\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    gradient_accumulation_steps=2\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer\n)\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:58:56.216245Z","iopub.execute_input":"2024-04-14T10:58:56.216548Z","iopub.status.idle":"2024-04-14T11:11:45.922573Z","shell.execute_reply.started":"2024-04-14T10:58:56.216518Z","shell.execute_reply":"2024-04-14T11:11:45.920702Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecaf19c69e91494fae36b2016d50ce54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39734f6061704d51b1ae9d82c6431ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393d9a59b0704f70808e91b9cd431954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ebff4cf5724e87872cdd421460d871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6efaa07e75d94698a7c7ba7c5ef11760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60afbf14b774402484977faa11c71213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1067b93727042f8a8e4aaf2c5259ba2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_105954-s9i5if0b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/spark5/huggingface/runs/s9i5if0b' target=\"_blank\">lilac-yogurt-12</a></strong> to <a href='https://wandb.ai/spark5/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/spark5/huggingface' target=\"_blank\">https://wandb.ai/spark5/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/spark5/huggingface/runs/s9i5if0b' target=\"_blank\">https://wandb.ai/spark5/huggingface/runs/s9i5if0b</a>"},"metadata":{}},{"name":"stderr","text":"You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1518/1518 11:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.277000</td>\n      <td>0.224114</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.165800</td>\n      <td>0.222762</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.090800</td>\n      <td>0.270117</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1198324310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Cleanup to free memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"],"ename":"NameError","evalue":"name 'gc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\n# Predicting and saving output\nleaderboard_dataset = Dataset.from_pandas(leaderboard_data)\nleaderboard_dataset = leaderboard_dataset.map(tokenize_function, batched=True)\npredictions = trainer.predict(leaderboard_dataset)\npreds = predictions.predictions.argmax(-1)\ndecoded_preds = label_encoder.inverse_transform(preds)\nleaderboard_data['Category'] = decoded_preds\nleaderboard_data.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T11:12:13.940476Z","iopub.execute_input":"2024-04-14T11:12:13.940863Z","iopub.status.idle":"2024-04-14T11:12:23.775534Z","shell.execute_reply.started":"2024-04-14T11:12:13.940827Z","shell.execute_reply":"2024-04-14T11:12:23.774358Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4bcf03315a94200b1148f6a8659e560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]}]}